{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/atinghosh/VAE-pytorch/blob/master/VAE_CNN_BCEloss.py\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from VAE_mnist_v1 import VAE_mnist\n",
    "from VAE_CIFAR_v1 import VAE_CIFAR_v1\n",
    "from VAE_CIFAR_v2 import VAE_CIFAR_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "CUDA = True\n",
    "SEED = 1\n",
    "BATCH_SIZE = 128\n",
    "LOG_INTERVAL = 100\n",
    "EPOCHS = 25\n",
    "no_of_sample = 10\n",
    "DATASET = 1\n",
    "\n",
    "\n",
    "ZDIMS = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./CIFAR10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "\n",
    "if DATASET == 0:\n",
    "    print(\"MNIST\")\n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist', train=True, download=True,transform=transforms.ToTensor()),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist', train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "elif DATASET == 1:\n",
    "    print(\"CIFAR\")\n",
    "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./CIFAR10', train=True, download=True,transform=transforms.ToTensor()),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./CIFAR10', train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    \n",
    "elif DATASET == 2:\n",
    "    print(\"CIFAR10_PROCESSED\")\n",
    "    processed_CIFAR10_data_train = datasets.ImageFolder(root='cifar10_processed_train/', transform=transforms.ToTensor())\n",
    "    processed_CIFAR10_data_test = datasets.ImageFolder(root='cifar10_processed_test/', transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(processed_CIFAR10_data_train, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(processed_CIFAR10_data_train, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "#     processed_CIFAR10_data = datasets.ImageFolder(root='cifar10_processed/', transform=transforms.ToTensor())\n",
    "\n",
    "#     loader = torch.utils.data.DataLoader(processed_CIFAR10_data, batch_size=1, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "#     image_list = []\n",
    "#     label_list = []\n",
    "#     for batch_ndx, sample in enumerate(loader):\n",
    "#         if batch_ndx == 0:\n",
    "#             print(\"loader\", sample)\n",
    "#         image_list.append(sample[0])\n",
    "#         label_list.append(sample[1])\n",
    "\n",
    "#     train_percentage = 0.8\n",
    "#     train_image_list = image_list[:int(train_percentage*len(image_list))]\n",
    "#     test_image_list = image_list[int(train_percentage*len(image_list)):]\n",
    "#     train_label_list = label_list[:int(train_percentage*len(label_list))]\n",
    "#     test_label_list = label_list[int(train_percentage*len(label_list)):]\n",
    "\n",
    "#     train_tensor_image = torch.stack(train_image_list)\n",
    "#     train_tensor_label = torch.stack(train_label_list)\n",
    "#     train_list = [train_tensor_image, train_tensor_label]\n",
    "#     train_loader = torch.utils.data.DataLoader(train_list, batch_size=1,shuffle=True,**kwargs)\n",
    "\n",
    "\n",
    "#     test_tensor_image = torch.stack(test_image_list)\n",
    "#     test_tensor_label = torch.stack(test_label_list)\n",
    "#     test_list = [test_tensor_image, test_tensor_label]\n",
    "#     test_loader = torch.utils.data.DataLoader(test_list, batch_size=1,shuffle=True,**kwargs)\n",
    "    \n",
    "#     for idx, sample in enumerate(test_loader):\n",
    "#         if idx == 0:\n",
    "#             print(\"train_loader\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 0:\n",
    "    print(\"MNIST\")\n",
    "    model = VAE_mnist(ZDIMS, BATCH_SIZE, no_of_sample)\n",
    "elif DATASET == 1:\n",
    "    print(\"CIFAR10\")\n",
    "    model = VAE_CIFAR_v1(ZDIMS, BATCH_SIZE, no_of_sample)\n",
    "elif DATASET == 2:\n",
    "    print(\"CIFAR10_PROCESSED\")\n",
    "    model = VAE_CIFAR_v2(ZDIMS, BATCH_SIZE, no_of_sample)\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.9f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # each data is of BATCH_SIZE (default 128) samples\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if CUDA:\n",
    "            # make sure this lives on the GPU\n",
    "            data = data.cuda()\n",
    "\n",
    "        # we're only going to infer, so no autograd at all required: volatile=True\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += model.loss_function(recon_batch, data, mu, logvar).item()\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            # for the first 128 batch of the epoch, show the first 8 input digits\n",
    "            # with right below them the reconstructed output digits\n",
    "            if DATASET == 0:\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                        recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.data.cpu(),\n",
    "                           './mnist/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "            elif DATASET == 1:\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                        recon_batch.view(BATCH_SIZE, 3, 32, 32)[:n]])\n",
    "                save_image(comparison.data.cpu(),\n",
    "                           './CIFAR10/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "            elif DATASET == 2:\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                        recon_batch.view(BATCH_SIZE, 3, 100, 100)[:n]])\n",
    "                save_image(comparison.data.cpu(),\n",
    "                           './cifar10_processed/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldmco\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.005441\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.004911\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.004808\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.004792\n",
      "====> Epoch: 1 Average loss: 0.004944606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldmco\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0049\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.004870\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.004835\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.004817\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.004835\n",
      "====> Epoch: 2 Average loss: 0.004824667\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.004859\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.004838\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.004780\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.004806\n",
      "====> Epoch: 3 Average loss: 0.004815027\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.004771\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.004837\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.004859\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.004754\n",
      "====> Epoch: 4 Average loss: 0.004806830\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.004808\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.004721\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.004757\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.004762\n",
      "====> Epoch: 5 Average loss: 0.004803484\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.004730\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.004829\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.004703\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.004771\n",
      "====> Epoch: 6 Average loss: 0.004799489\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.004812\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.004836\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.004806\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.004872\n",
      "====> Epoch: 7 Average loss: 0.004796285\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.004792\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.004780\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.004826\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.004748\n",
      "====> Epoch: 8 Average loss: 0.004792518\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.004835\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.004764\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.004763\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.004737\n",
      "====> Epoch: 9 Average loss: 0.004788054\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.004774\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.004816\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.004828\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.004852\n",
      "====> Epoch: 10 Average loss: 0.004784148\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.004723\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.004826\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.004700\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.004740\n",
      "====> Epoch: 11 Average loss: 0.004782665\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.004805\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.004715\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.004871\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.004749\n",
      "====> Epoch: 12 Average loss: 0.004781697\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.004780\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.004779\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.004723\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.004680\n",
      "====> Epoch: 13 Average loss: 0.004780883\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.004802\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.004744\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.004814\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.004734\n",
      "====> Epoch: 14 Average loss: 0.004778994\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.004741\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.004753\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.004873\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.004833\n",
      "====> Epoch: 15 Average loss: 0.004778194\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.004857\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.004720\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.004746\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.004737\n",
      "====> Epoch: 16 Average loss: 0.004777543\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.004769\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.004780\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.004796\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.004754\n",
      "====> Epoch: 17 Average loss: 0.004776549\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.004681\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.004795\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.004793\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.004752\n",
      "====> Epoch: 18 Average loss: 0.004776156\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.004700\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.004831\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.004836\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.004775\n",
      "====> Epoch: 19 Average loss: 0.004775799\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.004846\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.004882\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.004835\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.004816\n",
      "====> Epoch: 20 Average loss: 0.004775073\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.004736\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.004779\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.004753\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.004796\n",
      "====> Epoch: 21 Average loss: 0.004774539\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.004772\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.004717\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.004727\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.004778\n",
      "====> Epoch: 22 Average loss: 0.004774354\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.004744\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.004798\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.004663\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.004759\n",
      "====> Epoch: 23 Average loss: 0.004773495\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.004655\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.004720\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.004747\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.004725\n",
      "====> Epoch: 24 Average loss: 0.004773249\n",
      "====> Test set loss: 0.0048\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.004757\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.004854\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.004731\n",
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.004776\n",
      "====> Epoch: 25 Average loss: 0.004772928\n",
      "====> Test set loss: 0.0048\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    # 64 sets of random ZDIMS-float vectors, i.iie. 64 locations / MNIST\n",
    "    # digits in latent space\n",
    "    sample = Variable(torch.randn(64, ZDIMS))\n",
    "    if CUDA:\n",
    "        sample = sample.cuda()\n",
    "    sample = model.decode(sample).cpu()\n",
    "\n",
    "    # save out as an 8x8 matrix of MNIST`ii digits\n",
    "    # this will give you a visual idea of how well latent space can generate things\n",
    "    # that look like digits\n",
    "    if DATASET == 0:\n",
    "        save_image(sample.data.view(64, 1, 28, 28),'./mnist/reconstruction' + str(epoch) + '.png')\n",
    "    elif DATASET == 1:\n",
    "        save_image(sample.data.view(64, 3, 32, 32),'./CIFAR10/reconstruction' + str(epoch) + '.png')\n",
    "    elif DATASET == 2:\n",
    "        save_image(sample.data.view(64, 3, 100, 100),'./cifar10_processed/reconstruction' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"vae.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20])\n",
      "torch.Size([16, 3, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldmco\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "num_interpolation_points = 16\n",
    "sample = Variable(torch.randn(2, ZDIMS))\n",
    "first_point = sample[0]\n",
    "last_point = sample[1]\n",
    "interpolation_points_list = []\n",
    "for i in np.linspace(0,1,num_interpolation_points):\n",
    "    new_interpolation_point = (1-i)*first_point+i*last_point\n",
    "    interpolation_points_list.append(new_interpolation_point)\n",
    "\n",
    "interpolation_sample = Variable(torch.stack(interpolation_points_list))\n",
    "print(interpolation_sample.shape)\n",
    "if CUDA:\n",
    "    interpolation_sample = interpolation_sample.cuda()\n",
    "interpolation_sample = model.decode(interpolation_sample).cpu()\n",
    "print(interpolation_sample.shape)\n",
    "if DATASET == 0:\n",
    "    save_image(interpolation_sample.data.view(num_interpolation_points, 1, 28, 28),'./mnist/interpolation.png')\n",
    "else:\n",
    "    save_image(interpolation_sample.data.view(num_interpolation_points, 3, 32, 32),'./CIFAR10/interpolation.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
